{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyTVxl-ReYgW",
        "outputId": "07604835-6e06-4d01-da8d-864b95966156"
      },
      "outputs": [],
      "source": [
        "# INITIAL DATA CLEANING AND ENLISTING\n",
        "import pandas as pd\n",
        "import nltk\n",
        "# nltk.download('stopwords')\n",
        "# nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "# INCORPORATING DATASET FILE\n",
        "file = open('./disease_components.csv', 'rb')\n",
        "de = pd.read_csv(file, encoding='Windows-1252')\n",
        "filex = open('./disease_cleaned.csv', 'w')\n",
        "\n",
        "# adding new column and saving\n",
        "de[\"info-symptoms\"] = \"\"\n",
        "de[\"info-causes\"] = \"\"\n",
        "de[\"info-medication\"] = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icv5tJHbf1u1"
      },
      "outputs": [],
      "source": [
        "attributes = [\"Name\", \"Overview\", \"Symptoms\", \"Causes\", \"Risk factors\", \"Diagnosis\", \"Treatment\", \"Remedies\", \"Medication\"\n",
        "]\n",
        "bulletAttr = [\"Symptoms\", \"Causes\", \"Medication\"]\n",
        "for row in range(0, 1183):\n",
        "  for chosen in attributes:\n",
        "\n",
        "    # SKIP IF N/A\n",
        "    if pd.isna(de.loc[row, chosen]):\n",
        "      continue\n",
        "\n",
        "    # LOAD THE DATA\n",
        "    txt = de.loc[row, chosen]\n",
        "\n",
        "    # CHECK IF NAME\n",
        "    if chosen==\"Name\":\n",
        "      words = word_tokenize(txt)\n",
        "      final = []\n",
        "      for word in range(len(words)):\n",
        "        if (words[word] ==\"(\" and (words[word+1]==\"See\" or words[word+1]==\"see\")) or (\"See\" in words[word] or \"see\" in words[word]):\n",
        "          break\n",
        "        final.append(words[word])\n",
        "      de.loc[row, chosen] = ' '.join(final)\n",
        "      continue\n",
        "    \n",
        "    # CLEANING THE TEXT\n",
        "    txt = list(txt)\n",
        "    unwanted  = [\"[\", \"]\"]\n",
        "    apos = ['s', 'm', 'r', 't']\n",
        "    for char in range(len(txt)):\n",
        "      if txt[char] == \"$\" and (txt[char+1] == \",\" or txt[char+1] == \"\\\"\" or txt[char+1] == \"'\"):\n",
        "        txt[char+1] = \"$\"\n",
        "      if txt[char] == \",\" and (txt[char-1] == \".\" or txt[char-1] == \"\\\"\"):\n",
        "        txt[char] = \"$\"\n",
        "      if txt[char] in unwanted:\n",
        "        txt[char] = \"$\"\n",
        "      if txt[char] == \"'\" and char!=len(txt)-1:\n",
        "        if (txt[char+1] in apos):\n",
        "          g = 6\n",
        "        else: \n",
        "          txt[char] = \"$\"\n",
        "        if (txt[char+1]==\",\"):\n",
        "          txt[char+1] = \"$\"\n",
        "      if txt[char] == \"'\" and char==len(txt)-1:\n",
        "        txt[char] = \"$\"\n",
        "    txt = \"\".join(txt)\n",
        "    txt = txt.replace(\"\\\"\", \"\")\n",
        "    txt = txt.replace(\"''\", \"\")\n",
        "    txt = txt.replace(\"$\", \"\")\n",
        "    txt = txt.replace(\"\\\\n\", \"\")\n",
        "    txt = txt.replace(\"\\\\\", \"\")\n",
        "\n",
        "    # Stop-words\n",
        "    stopWords = list(stopwords.words(\"english\"))\n",
        "\n",
        "    # Remove duplicates and unwanted sentences\n",
        "    duplicates = []\n",
        "    cleaned = []\n",
        "    sentences = sent_tokenize(txt)\n",
        "    for s in sentences:\n",
        "      if chosen==\"Symptoms\" or chosen==\"Causes\": #or chosen==\"Causes\":\n",
        "        if len(word_tokenize(s))>30:\n",
        "          #print(\"Reomved: \", s)\n",
        "          continue\n",
        "      if s in cleaned:\n",
        "        if s in duplicates:\n",
        "            continue\n",
        "        else:\n",
        "            duplicates.append(s)\n",
        "      else:\n",
        "        if \"doi\" in s or \"Accessed\" in s or \"Mayo Clinic\" in s or \"http\" in s:\n",
        "          continue\n",
        "        else:\n",
        "          cleaned.append(s)\n",
        "\n",
        "    # New cleaned text\n",
        "    tx = \" \".join(cleaned)\n",
        "\n",
        "    # Tokenize into sentences\n",
        "    sent = sent_tokenize(tx)\n",
        "\n",
        "    # Getting attributes for the info-box\n",
        "    if chosen in bulletAttr:\n",
        "      bullets = []\n",
        "      for s in sent:\n",
        "        flag = 1\n",
        "        words = word_tokenize(s)\n",
        "        for w in words:\n",
        "          if w in stopWords:\n",
        "            flag = 0\n",
        "            break\n",
        "        if flag and len(words)<=3:\n",
        "          bullets.append(s)\n",
        "      \n",
        "      info = \"\"\n",
        "      if bullets:\n",
        "        for b in range(len(bullets)):\n",
        "          bullets[b] = bullets[b].replace(\".\", \"\")\n",
        "        info = \", \".join(bullets)\n",
        "      \n",
        "      if chosen==\"Symptoms\":\n",
        "        de.loc[row, \"info-symptoms\"] = info\n",
        "      elif chosen==\"Causes\":\n",
        "        de.loc[row, \"info-causes\"] = info\n",
        "      elif chosen==\"Medication\":\n",
        "        de.loc[row, \"info-medication\"] = info\n",
        "      print(chosen, \" : \", info)\n",
        "  \n",
        "    # Summarizing the paragraphs\n",
        "    words = word_tokenize(tx)\n",
        "   \n",
        "    # Creating a frequency table to keep the score of each word\n",
        "      \n",
        "    freqTable = dict()\n",
        "    for word in words:\n",
        "        word = word.lower()\n",
        "        if word in stopWords:\n",
        "            continue\n",
        "        if word in freqTable:\n",
        "            freqTable[word] += 1\n",
        "        else:\n",
        "            freqTable[word] = 1\n",
        "      \n",
        "    # Creating a dictionary to keep the score of each sentence\n",
        "    sentences = sent_tokenize(tx)\n",
        "    sentenceValue = dict()\n",
        "\n",
        "    if not sentences:\n",
        "      de.loc[row, chosen] = \"\"\n",
        "      continue\n",
        "      \n",
        "    for sentence in sentences:\n",
        "        for word, freq in freqTable.items():\n",
        "            if word in sentence.lower():\n",
        "                if sentence in sentenceValue:\n",
        "                    sentenceValue[sentence] += freq\n",
        "                else:\n",
        "                    sentenceValue[sentence] = freq\n",
        "      \n",
        "      \n",
        "      \n",
        "    sumValues = 0\n",
        "    for sentence in sentenceValue:\n",
        "        sumValues += sentenceValue[sentence]\n",
        "      \n",
        "    # Average value of a sentence from the original text\n",
        "    \n",
        "    average = int(sumValues / len(sentenceValue))\n",
        "      \n",
        "    # Storing sentences into our summary.\n",
        "    summary = ''\n",
        "    if chosen==\"Overview\":\n",
        "      summary += \" \" + sentences[0]\n",
        "    \n",
        "    factor = 1.3\n",
        "    if chosen==\"Symptoms\" or chosen==\"Causes\":\n",
        "      factor = 1.25\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if (sentence in sentenceValue) and (sentenceValue[sentence] > (factor * average)):\n",
        "          if chosen==\"Overview\" and sentence == sentences[0]:\n",
        "            continue\n",
        "          else:\n",
        "            summary += \" \" + sentence\n",
        "\n",
        "\n",
        "    # Structure the language of the summary\n",
        "    #summary = summary.lower()\n",
        "    summary = summary.replace(\"your\", \"the\")\n",
        "    summary = summary.replace(\"you're\", \"one is\")\n",
        "    summary = summary.replace(\"you are\", \"one is\")\n",
        "    summary = summary.replace(\"you have\", \"one has\")\n",
        "    summary = summary.replace(\"you'll\", \"one'll\")\n",
        "    summary = summary.replace(\"you\", \"one\")\n",
        "    summary = summary.replace(\"Your\", \"The\")\n",
        "    summary = summary.replace(\"You're\", \"One is\")\n",
        "    summary = summary.replace(\"You are\", \"One is\")\n",
        "    summary = summary.replace(\"You have\", \"One has\")\n",
        "    summary = summary.replace(\"You'll\", \"One'll\")\n",
        "    summary = summary.replace(\"You\", \"One\")\n",
        "    summary = summary.replace(\"Onell\", \"One will\")\n",
        "    summary = summary.replace(\"oneve\", \"one has\")\n",
        "    summary = summary.replace(\"onere\", \"one is\")\n",
        "    summary = summary.replace(\"onell\", \"one will\")\n",
        "    summary = summary.replace(\"Oneve\", \"One has\")\n",
        "    summary = summary.replace(\"Onere\", \"One is\")\n",
        "    summary = summary.replace(\"Oneng\", \"Young\")\n",
        "    summary = summary.replace(\"oneng\", \"young\")\n",
        "    summary = summary.replace(\"include:\", \"include\")\n",
        "    summary = summary.replace(\"including:\", \"including\")\n",
        "    summary = summary.replace(\"ï¿½\", \"\")\n",
        "    summary = summary.replace(\"try these tips:\", \"\")\n",
        "    summary = summary.strip()\n",
        "    summary = ' '.join(summary.split())\n",
        "    if \". \" not in summary:\n",
        "      summary = \"\"\n",
        "    # if (chosen==\"Symptoms\" or chosen==\"Causes\" or chosen==\"Risk Factors\") and summary!=\"\":\n",
        "    #   sts = sent_tokenize(summary)\n",
        "    #   for senten in sts:\n",
        "    #     words = word_tokenize(senten)\n",
        "    #     # print(words)\n",
        "    #     if len(words)>33:\n",
        "    #       summary = \"\"\n",
        "    #       break\n",
        "\n",
        "    # if chosen==\"Symptoms\" or chosen==\"Causes\":\n",
        "    print(row)\n",
        "    print(\"\\n\")\n",
        "\n",
        "    de.loc[row, chosen] = summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Saving the cleaned data\n",
        "de.to_csv(filex, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(de.loc[0, \"Symptoms\"])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "googleTrans",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
